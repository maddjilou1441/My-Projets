{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='blue'>\n",
    "- <font color='blue'> The motivation of this data-challenge project is to practice what we did in the Kernel Methods AMMI 2020 for machine learning. \n",
    "- <font color='blue'> The way we do the practice is to classify a data set about transcription factor. During this classification task we should predict whether a DNA sequence region is binding site to a specific transcription factor. \n",
    "- <font color='blue'> For that we have been provided 2000 data sample data labeled to learn our model which should be able to classifies as possible as 1000 sample unlabled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = 'blue'>               DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a3vR9rRC_QMm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kYF0R-IR_wk2"
   },
   "outputs": [],
   "source": [
    "df_Xtr = pd.read_csv('Xtr.csv') # Loading the training data \n",
    "df_Ytr = pd.read_csv('Ytr.csv') # Loading the training label data\n",
    "df_Xte = pd.read_csv('Xte.csv') # Loading the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GAGGGGCTGGGGAGGGGGCTGGCCCAGAGGCACCAGACTCTGCAGAACCACCCAGGCATTGTGGGGCTGCCCTGCCACCTGCTGGCCGCTCCTGGTGGCAG'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Xtr.iloc[0][1] # The information in the sample data of index 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='blue'> Here we are processing to numerize the training sequence data by replacing the characters A, C, G and G  with A=(1, 0, 0, 0), C=(0, 1, 0, 0), G=(0, 0, 1, 0), T=(0, 0, 0, 1) repectivly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gd5toL4N_3gn"
   },
   "outputs": [],
   "source": [
    "df = []\n",
    "for i in range(df_Xtr.shape[0]):\n",
    "    l = []\n",
    "    for j in range(len(df_Xtr.iloc[i][1])):\n",
    "        char = df_Xtr.iloc[i][1][j]\n",
    "        if char == 'A':\n",
    "            l = l+[1, 0, 0, 0]\n",
    "        elif char == 'C':\n",
    "            l = l+[0, 1, 0, 0]\n",
    "        elif char == 'G':\n",
    "            l = l+[0, 0, 1, 0]\n",
    "        else :\n",
    "            l = l+[0, 0, 0, 1]\n",
    "    df.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MlL9Kujb_9AI"
   },
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='blue'> The training Label data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_U4IxY1sEW2i"
   },
   "outputs": [],
   "source": [
    "y = df_Ytr['Bound'].values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='blue'> For the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mAIlyAyUEZvX"
   },
   "outputs": [],
   "source": [
    "df_t = []\n",
    "for i in range(df_Xte.shape[0]):\n",
    "    l = []\n",
    "    for j in range(len(df_Xte.iloc[i][1])):\n",
    "        char = df_Xte.iloc[i][1][j]\n",
    "        if char == 'A':\n",
    "            l = l+[1, 0, 0, 0]\n",
    "        elif char == 'C':\n",
    "            l = l+[0, 1, 0, 0]\n",
    "        elif char == 'G':\n",
    "            l = l+[0, 0, 1, 0]\n",
    "        else :\n",
    "            l = l+[0, 0, 0, 1]\n",
    "    df_t.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eetJRyH-EhVP"
   },
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame(df_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PLO5ssz0EtnX"
   },
   "outputs": [],
   "source": [
    "df_test = df_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color ='blue'> Here we convert the type data into float which is required down for the mathematical computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "px7xQae0Ewsk"
   },
   "outputs": [],
   "source": [
    "df_train = df_train.astype('float') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color ='blue'> Here is where we do feature that is the selection of the k best for training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Sgrmqq4Ezfa"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "_cCcme1JE25G",
    "outputId": "3d33fba1-a47b-4708-a4c3-d848ec4a7fe6"
   },
   "outputs": [],
   "source": [
    "test = SelectKBest(score_func=chi2, k=100)\n",
    "fit = test.fit(df_train, y)\n",
    "np.set_printoptions(precision=3)\n",
    "df_train = fit.transform(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "KbsdzWn8E-5n",
    "outputId": "31769819-0ddd-44b8-f6c4-03d183cd97e8"
   },
   "outputs": [],
   "source": [
    "df_test = fit.transform(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color ='blue'> Here we show the shape of the testing data that has 2000 rows and 404 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 100)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color ='blue'> Here we show the shape of the testing data that has 1000 rows and 404 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 100)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color ='blue'> The splitting of the training data into training data set and validation data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JxPX-H0pFCGW"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = 'blue'> keeping the y_train value before changing into a vectors of components (-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UzUZsZEnFFKJ"
   },
   "outputs": [],
   "source": [
    "y_train_keep = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "TVaew31DFJT3",
    "outputId": "293c41f5-aeae-4402-de62-6dc9407b9263"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n",
      "1500\n",
      "500\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "print(len(X_test))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = 'blue'> changing the y_train into a vectors of components (-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4AsRtWAaFMMs"
   },
   "outputs": [],
   "source": [
    "y_new = []\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i] == 0:\n",
    "        y_new.append(-1)\n",
    "    else:\n",
    "        y_new.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "tClOWEvn4qwJ",
    "outputId": "d2609265-244c-4ee9-d2e4-12d7966e5ee8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 0, 0, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "BAN6XxRaIVqd",
    "outputId": "4d4cb34e-eece-4aa1-97ae-8b3eaae96dc0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1,  1, -1, -1, -1,  1,  1,  1,  1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.array(y_new)\n",
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - <font color = 'blue'> Logistic Regression from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### - <font color = 'blue'> This functio return w, b the primal solutions from the dual results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_primal_from_dual(alpha, X, y, hard_margin=False, C=None, tol=1e-10):\n",
    "    # w parameter in vectorized form\n",
    "    w = ((y * alpha).T.dot(X)).flatten()\n",
    "    # w = X.dot(y[:, np.newaxis]*alphas)\n",
    "    #print(alpha)\n",
    "    # sv = Support vectors!\n",
    "    # Indices of points (support vectors) that lie exactly on the margin\n",
    "    # Filter out points with alpha == 0\n",
    "    sv = (alpha > tol)\n",
    "    # If soft margin, also filter out points with alpha == C\n",
    "    if not hard_margin:\n",
    "        if C is None:\n",
    "            raise ValueError('C must be defined in soft margin mode')\n",
    "            print(C)\n",
    "        sv = np.logical_and(sv, (C - alpha > tol))\n",
    "    #print('y',sv[:100])\n",
    "    b = y[sv] - X[sv].dot(w)\n",
    "    b = b[0]\n",
    "    \n",
    "    #Display results\n",
    "    # print('Alphas = {}'.format(alpha[sv]))\n",
    "    # print('Number of support vectors = {}'.format(sv.sum()))\n",
    "#     print('w = {}'.format(w))\n",
    "#     print('b = {}'.format(b))\n",
    "    \n",
    "    return w, b\n",
    "\n",
    "# w, b = get_primal_from_dual(alphas, X_train, y_train, hard_margin=True)\n",
    "# # plot_points_with_margin(X_train, y_train, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxopt\n",
    "solver='cvxopt'\n",
    "from cvxopt import matrix, solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadprog_solve_qp(P, q, G=None, h=None, A=None, b=None):\n",
    "    qp_G = .5 * (P + P.T)   # make sure P is symmetric\n",
    "    qp_a = -q\n",
    "    print(A)\n",
    "    if A is not None:\n",
    "        qp_C = -np.vstack([A, G]).T\n",
    "        qp_b = -np.hstack([b, h])\n",
    "        meq = A.shape[0]\n",
    "    else:  # no equality constraint\n",
    "        qp_C = - G.T\n",
    "        qp_b = - h\n",
    "        meq = 0\n",
    "    print(G.shape)\n",
    "    return quadprog.solve_qp(qp_G, qp_a, qp_C, qp_b, meq)[0]\n",
    "\n",
    "def cvxopt_qp(P, q, G, h, A, b):\n",
    "    P = .5 * (P + P.T)\n",
    "    # print(A.shape)\n",
    "    cvx_matrices = [\n",
    "        cvxopt.matrix(M) if M is not None else None for M in [P, q, G, h, A, b] \n",
    "    ]\n",
    "    solution = cvxopt.solvers.qp(*cvx_matrices)\n",
    "\n",
    "    return np.array(solution['x']).flatten()\n",
    "solve_qp = {'quadprog': quadprog_solve_qp, 'cvxopt': cvxopt_qp}[solver]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - <font color = 'blue'> Soft-margin svm learning step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - <font color = 'blue'> After trying the hard margin svm supposing the are linearly separable, I didn't get a good accuracy.\n",
    "    \n",
    "#### - <font color = 'blue'> For that thing I suppose that the soft-margin is better for this data set.\n",
    "#### - <font color = 'blue'> The objective is to maximise that soft-margin which is the same to minimize the objective function of the primal and maximize the ojective function with respect to their constraints. The primal and dual objective function for soft margin are define below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<font color = 'blue'>\n",
    "We will use a quadratic program (QP) solver `cvxopt` to find our own solution to SVM\n",
    "    \n",
    "\n",
    "```\n",
    "cvxopt.solvers.qp(P, q[, G, h[, A, b]])\n",
    "```\n",
    "    \n",
    "solves the quadratic program\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_x & \\, \\frac{1}{2}x^\\top P x + q^\\top x \\\\\n",
    "\\mathrm{s.t. } \\, & Gx \\leq h \\\\\n",
    "& Ax = b\n",
    "\\end{aligned}\n",
    "$$\n",
    "<font color = 'blue'>\n",
    "- $P, q$ define the objective\n",
    "- $G, h$ are all the inequality constraints\n",
    "- $A, b$ are all the equality constraints\n",
    "\n",
    "**Find $P$, $q$, $G$, $h$, $A$ and $b$ for the hard margin SVM**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='blue'> \n",
    "    \n",
    "#### - <font color='blue'> As we know on convex optimization problem, it is sometime simpler to get the solution from the dual than the primal.\n",
    "#### - <font color='blue'>That's we we define this function <font color='green'>'get_primal_from_dual' <font color='blue'> that we will call below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Soft margin SVM, primal:**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{w, b, \\xi} & \\, \\frac{1}{2}w^\\top w + C \\mathbf{1}^\\top \\xi \\\\\n",
    "\\mathrm{s.t. } \\, & \\xi \\geq 0 \\\\\n",
    "& y_i x_i^\\top w + y_i b + \\xi_i\\geq 1\n",
    "\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- <font color='green'> $P = \\frac{1}{2}I_p$ with 0 padding to make it $(p+1+n) \\times (p+1+n)$\n",
    "- $q = (0,..., 0, C, ..., C)$ </font> $q$ is $0$ $(p+1)$ times then $C$ $n$ times\n",
    "$$$$\n",
    "- <font color='green'> $G = -\\left[X_y^\\top, y, I_n\\right]^\\top, h = -\\mathbf{1}_n^\\top$</font>\n",
    "- <font color='green'> $A = 0, b = 0$ </font>\n",
    "\n",
    "**Soft margin SVM, dual:**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\max_\\alpha & \\, \\mathrm{1}^\\top\\alpha -\\frac{1}{2}\\alpha^\\top X_y^T X_y \\alpha \\\\\n",
    "\\mathrm{s.t. } \\, & \\alpha \\geq 0 \\\\\n",
    "& \\alpha \\leq C \\\\\n",
    "& y^\\top\\alpha = 0\n",
    "\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- <font color='green'> $P = -X_y^T X_y$, $q = -\\mathbf{1}$ </font>\n",
    "- <font color='green'> $G = [-I, I]^\\top, h = (0, ..., 0, C, ..., C)^\\top$</font> $h$ is $0$ $n$ times then $C$ $n$ times\n",
    "$$$$\n",
    "- <font color='green'> $A = y^\\top, b = 0$ </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = 'blue'>\n",
    "#### - <font color = 'blue'> This function the matrices P, G, A and the vectors q, h of the primal. \n",
    "#### - <font color = 'blue'> And finally we apply the solve_qp to return coefficient on which we use to primal optimal solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "5fXnD5OyG1ad",
    "outputId": "6a81677b-9499-4e30-b7c9-3f5216629675"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -8.8934e+04  5.7797e+04  2e+05  2e+01  4e+00\n",
      " 1:  2.6513e+04  6.0685e+03  2e+04  2e-15  6e-15\n",
      " 2:  1.2543e+04  7.2743e+03  5e+03  1e-15  3e-14\n",
      " 3:  1.1301e+04  8.1306e+03  3e+03  8e-16  3e-14\n",
      " 4:  1.0409e+04  8.6684e+03  2e+03  5e-16  2e-14\n",
      " 5:  9.8895e+03  9.0077e+03  9e+02  4e-16  2e-14\n",
      " 6:  9.6190e+03  9.1865e+03  4e+02  4e-16  3e-14\n",
      " 7:  9.4610e+03  9.2907e+03  2e+02  4e-16  8e-14\n",
      " 8:  9.4009e+03  9.3326e+03  7e+01  4e-16  6e-14\n",
      " 9:  9.3761e+03  9.3505e+03  3e+01  4e-16  1e-13\n",
      "10:  9.3687e+03  9.3560e+03  1e+01  4e-16  5e-13\n",
      "11:  9.3639e+03  9.3597e+03  4e+00  4e-16  3e-13\n",
      "12:  9.3620e+03  9.3613e+03  7e-01  4e-16  8e-13\n",
      "13:  9.3617e+03  9.3615e+03  1e-01  4e-16  3e-12\n",
      "14:  9.3616e+03  9.3616e+03  2e-02  4e-16  1e-11\n",
      "15:  9.3616e+03  9.3616e+03  7e-04  4e-16  2e-12\n",
      "Optimal solution found.\n"
     ]
    }
   ],
   "source": [
    "def svm_primal_soft_to_qp(X, y, C):\n",
    "    n, p = X.shape\n",
    "    assert (len(y) == n)\n",
    "    \n",
    "    Xy = np.diag(y).dot(X)\n",
    "    # Primal formulation, soft margin\n",
    "    diag_P = np.hstack([np.ones(p), np.zeros(n + 1)])\n",
    "    # As a regularization, we add epsilon * identity to P\n",
    "    eps = 1e-12\n",
    "    diag_P += eps\n",
    "    P = np.diag(diag_P)\n",
    "    \n",
    "    q = np.hstack([np.zeros(p + 1), C * np.ones(n)])\n",
    "    # y(wx+b)+ei>=1\n",
    "    G1 = - np.hstack([Xy, y[:, np.newaxis], np.eye(n)])\n",
    "    # ei>=0\n",
    "    G2 = - np.hstack([np.zeros((n, p+1)), np.eye(n)])\n",
    "    G = np.vstack([G1, G2])\n",
    "    h = - np.hstack([np.ones(n), np.zeros(n)])\n",
    "    A = None\n",
    "    b = None\n",
    "    return P, q, G, h, A, b\n",
    "C = 10\n",
    "coefs = solve_qp(*svm_primal_soft_to_qp(X_train, y_train, C=C))\n",
    "n, p = X_train.shape\n",
    "w_soft, b_soft, e = coefs[:p], coefs[p], coefs[(p+1):]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = 'blue'>\n",
    "#### - <font color = 'blue'> This function the matrices P, G, A and the vectors q, h of the dual. \n",
    "#### - <font color = 'blue'> And finally we apply the solve_qp to return alpha on which we use to get the optimal solution of the dual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "Kn8QCyVSHVji",
    "outputId": "286dd70c-10e8-44fd-c4b2-fc3329d2728e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -6.0128e+03 -1.1285e+05  2e+05  5e-01  1e-12\n",
      " 1: -6.0685e+03 -2.6513e+04  2e+04  1e-13  8e-13\n",
      " 2: -7.2743e+03 -1.2543e+04  5e+03  4e-14  9e-13\n",
      " 3: -8.1306e+03 -1.1301e+04  3e+03  6e-14  1e-12\n",
      " 4: -8.6684e+03 -1.0409e+04  2e+03  4e-13  1e-12\n",
      " 5: -9.0077e+03 -9.8895e+03  9e+02  9e-14  1e-12\n",
      " 6: -9.1865e+03 -9.6190e+03  4e+02  2e-13  1e-12\n",
      " 7: -9.2907e+03 -9.4610e+03  2e+02  1e-13  1e-12\n",
      " 8: -9.3326e+03 -9.4009e+03  7e+01  4e-15  1e-12\n",
      " 9: -9.3505e+03 -9.3761e+03  3e+01  2e-13  1e-12\n",
      "10: -9.3560e+03 -9.3687e+03  1e+01  4e-14  1e-12\n",
      "11: -9.3597e+03 -9.3639e+03  4e+00  3e-13  2e-12\n",
      "12: -9.3613e+03 -9.3620e+03  7e-01  3e-14  1e-12\n",
      "13: -9.3615e+03 -9.3617e+03  1e-01  3e-13  2e-12\n",
      "14: -9.3616e+03 -9.3616e+03  2e-02  4e-15  1e-12\n",
      "15: -9.3616e+03 -9.3616e+03  7e-04  2e-16  2e-12\n",
      "Optimal solution found.\n"
     ]
    }
   ],
   "source": [
    "def svm_dual_soft_to_qp(X, y, C):\n",
    "    n, p = X.shape\n",
    "    assert (len(y) == n)\n",
    "    \n",
    "    Xy = np.diag(y).dot(X)\n",
    "    # Dual formulation, soft margin\n",
    "    P = Xy.dot(Xy.T)\n",
    "    # As a regularization, we add epsilon * identity to P\n",
    "    eps = 1e-20\n",
    "    P += eps * np.eye(n)\n",
    "    q = - np.ones(n)\n",
    "    G = np.vstack([-np.eye(n), np.eye(n)])\n",
    "    h = np.hstack([np.zeros(n), C * np.ones(n)])\n",
    "    A = y[np.newaxis, :]\n",
    "    b = np.array([0.])\n",
    "    A = A.astype('float')\n",
    "    G = G.astype('float')\n",
    "    return P, q, G, h, A, b\n",
    "alphas = solve_qp(*svm_dual_soft_to_qp(X_train, y_train, C=C))\n",
    "# print(alphas.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = 'blue'>\n",
    "#### - <font color = 'blue'> Goal: find the best possible $w$ and $b$\n",
    "#### - <font color = 'blue'> Now we call the uncton <font color = 'green'>  'get_primal_from_dual' <font color = 'blue'> and apply it training data and the hyperparameters to find the weights corresponding to the optimal solutions. \n",
    "- <font color = 'blue'> Here the hyperparameters C is inversly propotional to the increasing of the overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "colab_type": "code",
    "id": "7QKbP1eFH79d",
    "outputId": "d609ac45-1df1-49ba-adff-fb5d31d545e4"
   },
   "outputs": [],
   "source": [
    "w_soft, b_soft =  get_primal_from_dual(alphas, X_train, y_train, C=C ) \n",
    "# plot_points_with_margin(X_train, y_train, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = 'blue'>\n",
    "#### - <font color = 'blue'> SVM as a model:\n",
    "#### - <font color = 'blue'> $\\hat{y} = sign(w^\\top x + b)$\n",
    "$$$$\n",
    "#### - <font color = 'blue'> Now we are making prediction <font color = 'green'> $\\hat{y}$ <font color = 'blue'> which tell us weither a data point in the training data or testing data is labeled by $1$ or $-1$ that means it is bound or not bound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GD0X5PCbH8TA"
   },
   "outputs": [],
   "source": [
    "def score_1(X, y, w, b):\n",
    "    # print(X_train.shape)\n",
    "    # print(w.shape)\n",
    "    prediction = np.sign(X@w+b)\n",
    "    # print(prediction[:4])\n",
    "    y_new = [1 if pred == 1 else 0 for pred in prediction]\n",
    "\n",
    "    sum_goodpred = 0\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        if y[i]==y_new[i]:\n",
    "            sum_goodpred+= 1\n",
    "    accur = sum_goodpred/len(y_new)\n",
    "    print('the accuracy for weight w, b is: {}%'.format((100*accur)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = 'blue'> Rechanging the y_train into vectors of components (0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "b8qUTX68FyBV",
    "outputId": "af0f57fe-0f6c-4734-c2d8-82fa57faa466"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 0, 0, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = y_train_keep \n",
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = 'blue'> Checking the accuracy with respect to the yperparameter C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy for weight w, b is: 71.2%\n"
     ]
    }
   ],
   "source": [
    "score_1(X_train, y_train, w_soft, b_soft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy for weight w, b is: 67.80000000000001%\n"
     ]
    }
   ],
   "source": [
    "score_1(X_test, y_test, w_soft, b_soft) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Final_Notebook_Data_challenge.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
